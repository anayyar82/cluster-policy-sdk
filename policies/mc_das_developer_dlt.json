# Databricks notebook source
from pyspark.sql.functions import lit, col

import dlt
import bronze_schema
import gold_schema

from variables_orig import *
from data_quality import *
from data_quality_gold import *

# COMMAND ----------

# MAGIC %md # Bronze Table

# COMMAND ----------

# MAGIC %md ### Blacklisted Accounts

# COMMAND ----------

@dlt.table(temporary=temporary)
def bronze_blacklist_account():
    return spark.read.table(f"`{marshall_catalog}`.`{misc_schema}`.`bl-accounts`")

# COMMAND ----------

# MAGIC %md ### Blacklist Gold

# COMMAND ----------

@dlt.table(
  name=bl_accounts_table_name + "GOLD",
  comment="blacklist gold table",
  schema=gold_schema.account_blacklist_schema
)
def gold_blacklist_account():
    return spark.sql("""
        SELECT 
            accountid AS account_id,
            reason,
            CAST(date_added AS DATE)
        FROM live.bronze_blacklist_account
    """)

# COMMAND ----------

# MAGIC %md ### Account Details

# COMMAND ----------

@dlt.table(temporary=temporary)
def account_details_input():
    return spark.read.table(f"`{dmz_catalog}`.`{input_schema}`.`account_details`")

@dlt.table(temporary=temporary)
def account_details_threshold():
    return spark.sql("""
        SELECT accountid 
        FROM live.account_details_input 
        WHERE devicenationality IN (
            SELECT devicenationality 
            FROM live.account_details_input 
            GROUP BY devicenationality 
            HAVING COUNT(devicenationality) < 11
        )
    """)

@dlt.table(temporary=temporary)
def account_details_without_blacklisted_users():
    return spark.sql("""
        SELECT * 
        FROM live.account_details_input 
        WHERE accountid NOT IN (SELECT accountid FROM live.bronze_blacklist_account)
    """)

@dlt.table(temporary=temporary)
def accounts_greater_than_threshold():
    return spark.sql("""
        SELECT * 
        FROM live.account_details_without_blacklisted_users
        WHERE accountid NOT IN (SELECT accountid FROM live.account_details_threshold)
    """)

@dlt.table(
    name="ACCOUNT_GOLD",
    schema=gold_schema.account_schema
)
def account_details_gold():
    return spark.sql("""
        SELECT 
            accountid AS account_id,
            devicenationality AS reader_nationality,
            CAST(agerangelower AS INTEGER) AS age_range_lower,
            CAST(agerangeupper AS INTEGER) AS age_range_upper
        FROM live.accounts_greater_than_threshold
        INNER JOIN live.whitelisted_countries
        ON live.accounts_greater_than_threshold.devicenationality = live.whitelisted_countries.code
    """)

# COMMAND ----------

# MAGIC %md ### Account Devices

# COMMAND ----------

@dlt.table(temporary=temporary)
def account_input():
    return (spark.read
            .table(f"`{dmz_catalog}`.`{input_schema}`.`account_devices`")
            .where(col("uploaddate").between(start_date, end_date))
            .withColumn("date_", lit(snapshot_date)))

@dlt.table(temporary=temporary)
def account_history():
    query = f"SHOW TABLES IN `{history_catalog}`.`{history_schema}` LIKE 'account_devices'"
    result = spark.sql(query)
    
    # Instead of collecting, check if the table exists
    if result.count() > 0:
        return spark.read.table(f"`{history_catalog}`.`{history_schema}`.`account_devices`")
    else:
        return spark.createDataFrame([], schema=bronze_schema.account_devices_schema)

@dlt.view()
def account_combined():
    account_input_df = spark.read.table("LIVE.account_input")
    account_history_df = spark.read.table("LIVE.account_history")
    
    # Repartition the data to avoid shuffle during the UNION operation
    combined_df = account_input_df.union(account_history_df)
    return combined_df.dropDuplicates(subset=['accountid', 'deviceuuid']).repartition("accountid")

@dlt.table(
    name=account_devices_table_name + "GOLD",
    comment="final account devices",
    schema=gold_schema.account_devices_schema
)
def account_final():
    return spark.sql("""
        SELECT 
            accountid AS account_id,
            deviceUUID AS reader_uuid,
            CAST(date_ AS DATE) AS first_processed_date  
        FROM LIVE.account_combined 
        WHERE deviceUUID IN (
            SELECT DISTINCT(reader_uuid) 
            FROM LIVE.DEVICE_SETTINGS_SILVER
        )
    """)

@dlt.table(
    name=account_devices_table_name + "BRONZE_HISTORY",
    comment="bronze account devices",
    temporary=temporary
)
def account_devices_update_history():
    df = spark.sql(f"""
        SELECT 
            ac.account_id AS accountid, 
            ac.reader_uuid AS deviceuuid, 
            ac.first_processed_date AS date_ 
        FROM live.{account_devices_table_name}GOLD ac 
        WHERE ac.first_processed_date = '{snapshot_date}'
    """)
    
    df.write.mode("append").format("delta")\
        .option('nullValue', None)\
        .option("mergeSchema", "true")\
        .saveAsTable(f"`{marshall_catalog}`.`{history_schema}`.`account_devices`")
    
    return df
