@dlt.view
#@dlt.table(temporary=temporary)
def current_filtered():
    #Formating Time
    time_df = spark.sql(f"select  if  (instr(right(userrecorded,6), '+') > 0 or instr(right(userrecorded,6), '-') > 0 , substr(userrecorded,0,length(userRecorded)-6) , userRecorded) as userRecorded_modified, * from LIVE.CURRENT_GLUCOSE_READING_BRONZE")

    time_df = time_df.drop("userrecorded") \
                    .withColumnRenamed("userrecorded_modified", "userrecorded") \
                    .withColumn('userrecorded', to_timestamp(col('userRecorded'))) \
                    .withColumn('factoryrecorded', to_timestamp(col('factoryrecorded')))
    return time_df

@dlt.table(
  name=current_table_name + "SILVER",
  comment="silver current",
  temporary=temporary)
def current_silver():
    new_df = spark.sql("SELECT * from LIVE.current_filtered;")
    new_df = new_df \
                        .withColumn("factoryrechour", hour(new_df.factoryrecorded)) \
                        .withColumn("userrechour", hour(new_df.userrecorded)) \
                        .withColumn("uploadSequence" ,col('uploadSequence').cast('long'))\
                        .withColumn("factoryrecorded" ,col('factoryrecorded').cast('timestamp'))
    new_df = new_df.withColumnRenamed("deviceUUID", "reader_uuid") \
                                        .withColumnRenamed("deviceNationality", "country") \
                                        .withColumnRenamed("uploadSequence", "upload_id") \
                                        .withColumnRenamed("firmwareVersion", "firmware")      
    new_df.orderBy(new_df['reader_uuid'].asc(), new_df['factoryrecorded'].asc())
    
    return new_df




from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, schema_of_json
 
df = spark.read.table("`prod-adc-superiorlv-catalog-eu-west-1`.`lv-event`.`bronze_eventlog`")
 
schema = schema_of_json("""{"deviceType":40066,"info":{"percent":100,"scans":18},"uploader":{"firstName":"Jens","id":"00c318d3-f467-e911-815d-0610e6e38cbd","lastName":"B�hme"}}""")
df_1 = df.withColumn("parsed_json", from_json(col('D'), schema))
df_1.select('D','parsed_json').display()


from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, schema_of_json

# Reading the table
df = spark.read.table("`prod-adc-superiorlv-catalog-eu-west-1`.`lv-event`.`bronze_eventlog`")

# Defining schema, ensuring special characters are correct
json_schema = """
{
  "deviceType": 40066,
  "info": {
    "percent": 100,
    "scans": 18
  },
  "uploader": {
    "firstName": "Jens",
    "id": "00c318d3-f467-e911-815d-0610e6e38cbd",
    "lastName": "Böhme"
  }
}
"""

# Defining schema from JSON string
schema = schema_of_json(json_schema)

# Parsing the JSON from column 'D'
df_1 = df.withColumn("parsed_json", from_json(col("D"), schema))

# Select and display original and parsed columns
df_1.select('D', 'parsed_json').display()


{
    "autoscale.max_workers": {
      "defaultValue": 3,
      "maxValue": 10,
      "type": "range"
    },
    "autoscale.min_workers": {
      "hidden": true,
      "type": "fixed",
      "value": 1
    },
    "autotermination_minutes": {
      "hidden": true,
      "type": "fixed",
      "value": 0
    },
    "aws_attributes.availability": {
      "hidden": true,
      "type": "fixed",
      "value": "SPOT_WITH_FALLBACK"
    },
    "aws_attributes.first_on_demand": {
      "defaultValue": 100001,
      "minValue": 1,
      "type": "range"
    },
    "aws_attributes.instance_profile_arn": {
      "hidden": false,
      "type": "fixed",
      "value": "arn:aws:iam::188683556025:instance-profile/SC-188683556025-pp-odoyentbc6yei-InstanceProfile-OH0rrAA6Cdlz"
    },
    "aws_attributes.spot_bid_price_percent": {
      "hidden": true,
      "type": "fixed",
      "value": 100
    },
    "aws_attributes.zone_id": {
      "defaultValue": "auto",
      "hidden": true,
      "type": "unlimited"
    },
    "cluster_type": {
      "type": "fixed",
      "value": "dlt"
    },
    "custom_tags.cluster-type": {
      "type": "fixed",
      "value": "dlt"
    },
    "custom_tags.project": {
      "isOptional": false,
      "type": "unlimited"
    },
    "custom_tags.use-case": {
      "defaultValue": "SpendingPulse, SBCA, CustomerDataLandingZone",
      "type": "allowlist",
      "values": [
        "Data Engineering",
        "Machine Learning",
        "Data Science",
        "Operations",
        "Partner Engineering"
      ]
    },
    "custom_tags.COST_CENTER": {
      "type": "fixed",
      "value": "CC8801"
     },
    "data_security_mode": {
      "defaultValue": "NONE",
      "hidden": true,
      "type": "unlimited"
    },
    "dbus_per_hour": {
      "maxValue": 50,
      "type": "range"
    },
    "driver_instance_pool_id": {
      "hidden": true,
      "type": "forbidden"
    },
    "driver_node_type_id": {
      "hidden": true,
      "type": "unlimited"
    },
    "enable_elastic_disk": {
      "hidden": true,
      "type": "fixed",
      "value": true
    },
    "instance_pool_id": {
      "hidden": true,
      "type": "forbidden"
    },
    "node_type_id": {
      "hidden": true,
      "type": "unlimited"
    },
    "num_workers": {
      "hidden": true,
      "type": "forbidden"
    },
    "runtime_engine": {
      "hidden": true,
      "type": "fixed",
      "value": "STANDARD"
    },
    "spark_conf.spark.databricks.cluster.profile": {
      "hidden": true,
      "type": "forbidden"
    },
    "spark_version": {
      "defaultValue": "auto:latest-lts",
      "type": "unlimited"
    }
  }
